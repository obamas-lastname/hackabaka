{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e548f8a1",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸš€ Hackathon Demo â€” Realâ€‘time Fraud Detection (Historyâ€‘Aware)\n",
    "\n",
    "**Agenda**\n",
    "1. Load a tiny sample of the dataset (pipeâ€‘delimited).\n",
    "2. Build *historyâ€‘aware* features (velocity, seenâ€‘before, time since last, distancesâ€¦).\n",
    "3. Train a baseline classifier (RandomForest).\n",
    "4. Pick an operating **threshold** from the Precisionâ€‘Recall curve (aim â†‘recall).\n",
    "5. Evaluate: Accuracy / Precision / Recall / F1 + Confusion Matrix.\n",
    "6. Save `model.pkl` + `threshold.json` for the API/streamer.\n",
    "7. (Optional) Live predict: call local FastAPI `/predict` or run a oneâ€‘off prediction.\n",
    "\n",
    "> **Tip:** Keep the CSV small in the notebook (e.g., 50k rows) so it runs snappy for the demo. Train the full model offline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f63d320",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mensemble\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "\n",
    "# %% Imports\n",
    "import os, json, math, sqlite3, statistics, datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, precision_recall_curve,\n",
    "    average_precision_score\n",
    ")\n",
    "\n",
    "pd.set_option('display.max_columns', 120)\n",
    "pd.set_option('display.width', 160)\n",
    "\n",
    "# Paths (edit for your machine)\n",
    "DATA_PATH = os.getenv(\"DATA_PATH\", \"transactions.csv\")  # pipe-delimited\n",
    "MODEL_PATH = \"model.pkl\"\n",
    "FEATURES_PATH = \"features.json\"\n",
    "THRESHOLD_PATH = \"threshold.json\"\n",
    "\n",
    "# For the live API demo (optional)\n",
    "FASTAPI_URL = os.getenv(\"FASTAPI_URL\", \"http://127.0.0.1:8000/predict?store=0\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19615b21",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Load a small, timeâ€‘ordered sample\n",
    "> For a smooth demo, read up to ~50k rows and **sort by `unix_time`** so our features only see the *past*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4991b9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% Load a small sample and sort by time\n",
    "use_rows = int(os.getenv(\"NB_DEMO_ROWS\", \"50000\"))  # tweak live if needed\n",
    "df = pd.read_csv(DATA_PATH, sep=\"|\", nrows=use_rows)\n",
    "\n",
    "# Ensure proper dtypes\n",
    "for col in [\"amt\", \"lat\", \"long\", \"merch_lat\", \"merch_long\", \"city_pop\", \"unix_time\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "# Sort by time\n",
    "if \"unix_time\" in df.columns:\n",
    "    df = df.sort_values(\"unix_time\").reset_index(drop=True)\n",
    "\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc360ea",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Feature engineering (historyâ€‘aware, lightweight for demo)\n",
    "\n",
    "This cell implements a *fast* feature builder meant for demos. It uses vectorized pandas where possible and perâ€‘row operations only for a few history lookups limited to the last 24h for each `cc_num`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5375a231",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% Feature helpers (fast-ish, pandas-based)\n",
    "\n",
    "def fast_age(dob_str, at_unix):\n",
    "    try:\n",
    "        y, m, d = map(int, str(dob_str)[:10].split(\"-\"))\n",
    "        t = dt.datetime.utcfromtimestamp(int(at_unix))\n",
    "        age = t.year - y - ((t.month, t.day) < (m, d))\n",
    "        return max(0, age)\n",
    "    except Exception:\n",
    "        return -1\n",
    "\n",
    "def hour_from_unix(unix_time):\n",
    "    try:\n",
    "        return dt.datetime.utcfromtimestamp(int(unix_time)).hour\n",
    "    except Exception:\n",
    "        return -1\n",
    "\n",
    "def dow_from_unix(unix_time):\n",
    "    try:\n",
    "        return dt.datetime.utcfromtimestamp(int(unix_time)).weekday()  # 0..6 (Mon..Sun)\n",
    "    except Exception:\n",
    "        return -1\n",
    "\n",
    "def is_night(h):\n",
    "    return 1 if (h <= 6 or h >= 22) else 0\n",
    "\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    try:\n",
    "        R = 6371.0\n",
    "        phi1 = math.radians(float(lat1)); phi2 = math.radians(float(lat2))\n",
    "        dphi = math.radians(float(lat2) - float(lat1))\n",
    "        dlmb = math.radians(float(lon2) - float(lon1))\n",
    "        a = math.sin(dphi/2)**2 + math.cos(phi1)*math.cos(phi2)*math.sin(dlmb/2)**2\n",
    "        return 2 * R * math.asin(math.sqrt(a))\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "# Build base columns\n",
    "def add_base_columns(df):\n",
    "    df = df.copy()\n",
    "    df[\"age\"] = [fast_age(d, u) for d, u in zip(df.get(\"dob\", \"\"), df[\"unix_time\"])]\n",
    "    df[\"hour\"] = [hour_from_unix(u) for u in df[\"unix_time\"]]\n",
    "    df[\"dow\"] = [dow_from_unix(u) for u in df[\"unix_time\"]]\n",
    "    df[\"is_night\"] = df[\"hour\"].apply(is_night)\n",
    "    df[\"log_amt\"] = np.log1p(df[\"amt\"].fillna(0.0))\n",
    "    for col in [\"lat\",\"long\",\"merch_lat\",\"merch_long\",\"city_pop\"]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0.0\n",
    "        df[col] = df[col].fillna(0.0)\n",
    "    # simple gender one-hot\n",
    "    g = df.get(\"gender\",\"\").astype(str).str.upper()\n",
    "    df[\"gender_M\"] = (g == \"M\").astype(int)\n",
    "    df[\"gender_F\"] = (g == \"F\").astype(int)\n",
    "    return df\n",
    "\n",
    "# History features per cc_num using last 24h window\n",
    "def add_history_features(df):\n",
    "    df = df.copy()\n",
    "    # Pre-allocate columns\n",
    "    cols = [\"velocity_60s\",\"velocity_5m\",\"velocity_15m\",\"velocity_1h\",\n",
    "            \"unique_merchants_15m\",\"unique_categories_15m\",\n",
    "            \"seen_merchant_before\",\"user_merchant_dist_km\",\n",
    "            \"time_since_last_s\",\"time_since_last_merchant_s\",\n",
    "            \"user_mean_amt_24h\",\"user_std_amt_24h\",\"user_amt_delta\",\"amt_z_user\"]\n",
    "    for c in cols: df[c] = 0.0\n",
    "\n",
    "    # Group by cc_num to ensure \"past only\" within each user\n",
    "    for cc, g in df.groupby(\"cc_num\", sort=False):\n",
    "        idx = g.index.values\n",
    "        times = g[\"unix_time\"].values.astype(np.int64)\n",
    "        amts = g[\"amt\"].fillna(0.0).values.astype(float)\n",
    "        merch = g[\"merchant\"].astype(str).values\n",
    "\n",
    "        # For quick lookup\n",
    "        lat_user = g[\"lat\"].values; lon_user = g[\"long\"].values\n",
    "        lat_m = g[\"merch_lat\"].values; lon_m = g[\"merch_long\"].values\n",
    "\n",
    "        # We'll walk forward and use a small sliding window index\n",
    "        start = 0\n",
    "        for i in range(len(g)):\n",
    "            t = times[i]\n",
    "            # keep last 24h window\n",
    "            while start < i and times[start] < t - 24*3600:\n",
    "                start += 1\n",
    "            window_slice = slice(start, i)  # strictly past rows\n",
    "            # velocity counts\n",
    "            v60  = np.sum(times[window_slice] >= t - 60)\n",
    "            v5   = np.sum(times[window_slice] >= t - 5*60)\n",
    "            v15  = np.sum(times[window_slice] >= t - 15*60)\n",
    "            v1h  = np.sum(times[window_slice] >= t - 60*60)\n",
    "\n",
    "            # uniques in last 15m\n",
    "            mask15 = times[window_slice] >= t - 15*60\n",
    "            uniq_merch = len(set(merch[window_slice][mask15]))\n",
    "            uniq_cat   = len(set(g[\"category\"].astype(str).values[window_slice][mask15]))\n",
    "\n",
    "            # seen merchant before?\n",
    "            seen_before = 1 if (merch[i] in set(merch[window_slice])) else 0\n",
    "\n",
    "            # time since last overall & same merchant\n",
    "            last_time = times[i-1] if i > 0 else None\n",
    "            if last_time is None:\n",
    "                tsl = 10**9\n",
    "            else:\n",
    "                tsl = t - last_time\n",
    "\n",
    "            # last same-merchant time\n",
    "            prev_same = None\n",
    "            for j in range(i-1, start-1, -1):\n",
    "                if merch[j] == merch[i]:\n",
    "                    prev_same = times[j]; break\n",
    "            tslm = (t - prev_same) if prev_same is not None else 10**9\n",
    "\n",
    "            # distances\n",
    "            # previous merchant to current merchant (fallback to user coords if needed)\n",
    "            if i > 0:\n",
    "                mlat_prev = lat_m[i-1] if not np.isnan(lat_m[i-1]) else lat_user[i-1]\n",
    "                mlon_prev = lon_m[i-1] if not np.isnan(lon_m[i-1]) else lon_user[i-1]\n",
    "                dist_prev_to_now = haversine_km(mlat_prev, mlon_prev,\n",
    "                                                lat_m[i] if not np.isnan(lat_m[i]) else lat_user[i],\n",
    "                                                lon_m[i] if not np.isnan(lon_m[i]) else lon_user[i])\n",
    "            else:\n",
    "                dist_prev_to_now = 0.0\n",
    "\n",
    "            # user->merchant distance for current row\n",
    "            dist_user_to_merchant = haversine_km(lat_user[i], lon_user[i], lat_m[i], lon_m[i])\n",
    "            if np.isnan(dist_user_to_merchant): dist_user_to_merchant = 0.0\n",
    "\n",
    "            # amount profile 24h\n",
    "            past_amts = amts[window_slice]\n",
    "            if past_amts.size:\n",
    "                mean24 = float(past_amts.mean())\n",
    "                std24  = float(past_amts.std(ddof=0))\n",
    "            else:\n",
    "                mean24, std24 = 0.0, 0.0\n",
    "            delta = float(amts[i] - mean24)\n",
    "            z = float(delta / std24) if std24 > 0 else 0.0\n",
    "\n",
    "            # assign\n",
    "            df.loc[idx[i], [\"velocity_60s\",\"velocity_5m\",\"velocity_15m\",\"velocity_1h\"]] = [v60, v5, v15, v1h]\n",
    "            df.loc[idx[i], [\"unique_merchants_15m\",\"unique_categories_15m\"]] = [uniq_merch, uniq_cat]\n",
    "            df.loc[idx[i], \"seen_merchant_before\"] = float(seen_before)\n",
    "            df.loc[idx[i], \"user_merchant_dist_km\"] = float(dist_prev_to_now if not np.isnan(dist_prev_to_now) else 0.0)\n",
    "            df.loc[idx[i], \"time_since_last_s\"] = float(tsl)\n",
    "            df.loc[idx[i], \"time_since_last_merchant_s\"] = float(tslm)\n",
    "            df.loc[idx[i], [\"user_mean_amt_24h\",\"user_std_amt_24h\",\"user_amt_delta\",\"amt_z_user\"]] = [mean24, std24, delta, z]\n",
    "\n",
    "    return df\n",
    "\n",
    "# Feature order for the model\n",
    "FEATURE_ORDER = [\n",
    "    \"age\",\"log_amt\",\"hour\",\"dow\",\"is_night\",\n",
    "    \"city_pop\",\"lat\",\"long\",\"merch_lat\",\"merch_long\",\n",
    "    \"velocity_60s\",\"velocity_5m\",\"velocity_15m\",\"velocity_1h\",\n",
    "    \"unique_merchants_15m\",\"unique_categories_15m\",\n",
    "    \"seen_merchant_before\",\"user_merchant_dist_km\",\n",
    "    \"time_since_last_s\",\"time_since_last_merchant_s\",\n",
    "    \"user_mean_amt_24h\",\"user_std_amt_24h\",\"user_amt_delta\",\"amt_z_user\",\n",
    "    \"gender_M\",\"gender_F\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5700e5",
   "metadata": {},
   "source": [
    "\n",
    "### Build the feature matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca4a740",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% Build features\n",
    "base = add_base_columns(df)\n",
    "feat_df = add_history_features(base)\n",
    "\n",
    "# Ensure target is numeric 0/1\n",
    "if \"is_fraud\" in feat_df.columns:\n",
    "    y = feat_df[\"is_fraud\"].astype(str).str.lower().map(\n",
    "        {\"1\":1,\"0\":0,\"true\":1,\"false\":0,\"t\":1,\"f\":0,\"yes\":1,\"no\":0}\n",
    "    ).fillna(0).astype(int)\n",
    "else:\n",
    "    raise ValueError(\"Column 'is_fraud' not found.\")\n",
    "\n",
    "X = feat_df[FEATURE_ORDER].fillna(0.0).astype(float).values\n",
    "X.shape, y.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8cc549",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Train / Evaluate (80/20) and choose threshold from PRâ€‘curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddfa8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y if y.sum() and y.sum()!=len(y) else None\n",
    ")\n",
    "\n",
    "# Model (heavier positive class weight to lift recall)\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=160,\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=1,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    class_weight={0:1.0, 1:5.0}\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_prob = model.predict_proba(X_test)[:,1]\n",
    "y_pred_50 = (y_prob >= 0.50).astype(int)\n",
    "\n",
    "print(\"=== Metrics @ 0.50 ===\")\n",
    "print(classification_report(y_test, y_pred_50, digits=4))\n",
    "print(\"Confusion matrix @ 0.50:\")\n",
    "print(confusion_matrix(y_test, y_pred_50))\n",
    "\n",
    "# PR curve and threshold suggestion (maximize F1)\n",
    "prec, rec, thr = precision_recall_curve(y_test, y_prob)\n",
    "f1 = (2*prec*rec) / (prec + rec + 1e-12)\n",
    "best_idx = np.argmax(f1)\n",
    "best_thr = thr[best_idx] if best_idx < len(thr) else 0.5\n",
    "print(f\"Suggested threshold (max F1): {best_thr:.3f}\")\n",
    "print(f\"PR-AUC: {average_precision_score(y_test, y_prob):.4f}\")\n",
    "\n",
    "# Plot PR\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(rec, prec, lw=2)\n",
    "plt.scatter(rec[best_idx], prec[best_idx], s=40)\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Metrics at suggested threshold\n",
    "y_pred_best = (y_prob >= best_thr).astype(int)\n",
    "print(\"=== Metrics @ best threshold ===\")\n",
    "print(classification_report(y_test, y_pred_best, digits=4))\n",
    "print(\"Confusion matrix @ best threshold:\")\n",
    "print(confusion_matrix(y_test, y_pred_best))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c608375b",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Save artifacts (model, features, threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98f183f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import joblib, json\n",
    "\n",
    "joblib.dump(model, MODEL_PATH)\n",
    "with open(FEATURES_PATH, \"w\") as f:\n",
    "    json.dump(FEATURE_ORDER, f, indent=2)\n",
    "with open(THRESHOLD_PATH, \"w\") as f:\n",
    "    json.dump({\"threshold\": float(best_thr)}, f, indent=2)\n",
    "\n",
    "print(\"Saved:\", MODEL_PATH, FEATURES_PATH, THRESHOLD_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09ec088",
   "metadata": {},
   "source": [
    "\n",
    "## 5) (Optional) Live predict via FastAPI\n",
    "\n",
    "Start your API in a terminal:\n",
    "\n",
    "```bash\n",
    "uvicorn fraud_api:app --host 127.0.0.1 --port 8000\n",
    "```\n",
    "\n",
    "Then run this cell to send one transaction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26302c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% Live predict demo (optional)\n",
    "import requests, json as _json\n",
    "\n",
    "if os.environ.get(\"ENABLE_LIVE_DEMO\", \"0\") == \"1\":\n",
    "    sample_tx = df.iloc[len(df)//2].to_dict()\n",
    "    try:\n",
    "        r = requests.post(FASTAPI_URL, json=sample_tx, timeout=5)\n",
    "        print(\"HTTP\", r.status_code)\n",
    "        print(r.text[:500])\n",
    "        try:\n",
    "            print(_json.dumps(r.json(), indent=2))\n",
    "        except Exception:\n",
    "            pass\n",
    "    except Exception as e:\n",
    "        print(\"Live demo skipped / failed:\", e)\n",
    "else:\n",
    "    print(\"Set ENABLE_LIVE_DEMO=1 to call the API from the notebook.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0b4f1a",
   "metadata": {},
   "source": [
    "\n",
    "## Appendix â€” Feature list\n",
    "```\n",
    "- age\n",
    "- log_amt\n",
    "- hour\n",
    "- dow\n",
    "- is_night\n",
    "- city_pop\n",
    "- lat\n",
    "- long\n",
    "- merch_lat\n",
    "- merch_long\n",
    "- velocity_60s\n",
    "- velocity_5m\n",
    "- velocity_15m\n",
    "- velocity_1h\n",
    "- unique_merchants_15m\n",
    "- unique_categories_15m\n",
    "- seen_merchant_before\n",
    "- user_merchant_dist_km\n",
    "- time_since_last_s\n",
    "- time_since_last_merchant_s\n",
    "- user_mean_amt_24h\n",
    "- user_std_amt_24h\n",
    "- user_amt_delta\n",
    "- amt_z_user\n",
    "- gender_M\n",
    "- gender_F\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
